{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-71835407cde0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "#Generates ratings for problems and users\n",
    "#then does some analysis on it\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from ggplot import *\n",
    "import datetime\n",
    "NROWS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '''/w/225/1/chess/tactics/'''\n",
    "os.chdir(path)\n",
    "filename = '''user_tactics_problem.csv_00'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Player:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rating = 1720\n",
    "        self.rd = 350\n",
    "        self.p = 3 * (math.log(10)**2) / ((math.pi ** 2) * 400**2)\n",
    "        self.q = math.log(10)/400\n",
    "        self.c = 900\n",
    "    \n",
    "    def getRating(self):\n",
    "        return self.rating\n",
    "    \n",
    "    def getRd(self):\n",
    "        return self.rd\n",
    "    \n",
    "    def f(self, oppRd):\n",
    "        return 1/math.sqrt(1 + (self.p * (oppRd ** 2)))\n",
    "    \n",
    "    def E(self, oppRating, oppRd):\n",
    "        return 1/(1 + 10**(-1 * (self.rating - oppRating) * self.f(oppRd) / 400))\n",
    "    \n",
    "    def K(self, oppRating, oppRd):\n",
    "        k = self.q * self.f(oppRd)/ ((1/self.rd ** 2) + (self.q **2)*(self.f(oppRd)**2)*self.E(oppRating, oppRd) * (1-self.E(oppRating, oppRd)))\n",
    "        if k < 16:\n",
    "            return 16\n",
    "        else:\n",
    "            return k\n",
    "    \n",
    "    def update_no_games(self, timeWithoutPlaying):\n",
    "        self.rd = math.sqrt(self.rd**2 + self.c * timeWithoutPlaying)\n",
    "        if self.rd > 350:\n",
    "            self.rd = 350\n",
    "    \n",
    "    def update_player(self, oppRating, oppRd, result):\n",
    "        self.rating = self.rating + (self.K(oppRating, oppRd) * (int(result) - self.E(oppRating, oppRd)))\n",
    "        self.rd = (math.sqrt((1/self.rd ** 2) + (self.q ** 2 ) * (self.f(oppRd) ** 2) * self.E(oppRating, oppRd) * (1-self.E(oppRating, oppRd))))** -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#step one, get \"difficulty rating\" for each problem, user.\n",
    "#ELO code borrowed from: https://www.geeksforgeeks.org/elo-rating-algorithm/\n",
    " \n",
    "# Function to calculate the Probability\n",
    "def Probability(rating1, rating2):\n",
    "    return 1.0 * 1.0 / (1 + 1.0 * math.pow(10, 1.0 * (rating1 - rating2) / 400))\n",
    " \n",
    "# Function to calculate Elo rating\n",
    "# K is a constant.\n",
    "# d determines whether\n",
    "# Player A wins or Player B. \n",
    "def EloRating(Ra, Rb, d):\n",
    "  \n",
    "    K = 30 #constant\n",
    "    # To calculate the Winning\n",
    "    # Probability of Player B\n",
    "    Pb = Probability(Ra, Rb)\n",
    " \n",
    "    # To calculate the Winning\n",
    "    # Probability of Player A\n",
    "    Pa = Probability(Rb, Ra)\n",
    " \n",
    "    # Case -1 When Player A wins\n",
    "    # Updating the Elo Ratings\n",
    "    if (d == 1) :\n",
    "        Ra = Ra + K * (1 - Pa)\n",
    "        Rb = Rb + K * (0 - Pb)\n",
    "     \n",
    " \n",
    "    # Case -2 When Player B wins\n",
    "    # Updating the Elo Ratings\n",
    "    else :\n",
    "        Ra = Ra + K * (0 - Pa)\n",
    "        Rb = Rb + K * (1 - Pb)\n",
    "    return Ra, Rb\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(filename, nrows = 10)\n",
    "#print(df)\n",
    "\n",
    "userDict = {} #user_ID:Rating\n",
    "problemDict = {} # ID:Rating\n",
    "userGamesPlayed = {} #user_ID:number of games played\n",
    "userLastPlayed = {} #user_ID:date of last game played\n",
    "\n",
    "'''\n",
    "def getRating(user, Dict, default):\n",
    "    if user not in Dict:\n",
    "        Dict[user] = default\n",
    "    return Dict[user]    \n",
    "'''\n",
    "def getRating(user, Dict):\n",
    "    if user not in Dict:\n",
    "        Dict[user] = Player()\n",
    "    return Dict[user].getRating()\n",
    "    \n",
    "\n",
    "def getDateTime(raw_dt):\n",
    "    a, b, c = [int(i) for i in raw_dt.split()[0].split(\"-\")]\n",
    "    return datetime.date(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save ints, not floats\n",
    "#write to file every 100000 lines or so to speed up\n",
    "#write own \n",
    "\n",
    "chunksUsed = 0\n",
    "newCSV = open(\"glicko_user_tactics_problem.csv_00\", \"w\", newline= \"\")\n",
    "newCSV = csv.writer(newCSV)\n",
    "newCSV.writerow([\"user_tactics_problem_id\", \"user_hash\", \"create_date\", \"date\", \"last_game_date\", \"seconds\", \"is_passed\", \"correct_move_count\", \"rating_change\", \"tactics_problem_id\", \"ratingUser\", \"varianceUser\", \"ratingProblem\", \"varianceProblem\", \"userGamesPlayed\"])\n",
    "problemProblems = []\n",
    "for chunk in pd.read_csv(filename, chunksize=10000): \n",
    "    rows = []\n",
    "    for index, row in chunk.iterrows():\n",
    "        user = row[\"user_hash\"]\n",
    "        problem = row[\"tactics_problem_id\"]\n",
    "        \n",
    "        if str(problem) in problemProblems:\n",
    "            continue\n",
    "        winner = row[\"is_passed\"]\n",
    "        date = getDateTime(row[\"create_date\"])\n",
    "        if user not in userLastPlayed:\n",
    "            userLastPlayed[user] = date\n",
    "        userRating = getRating(user, userDict)\n",
    "        daysMissed = (date-userLastPlayed[user]).days\n",
    "        userDict[user].update_no_games(daysMissed)\n",
    "        lastGame = userLastPlayed[user]\n",
    "        userLastPlayed[user] = date\n",
    "        userRd = userDict[user].getRd()\n",
    "        problemRating = getRating(problem, problemDict)\n",
    "        problemRd = problemDict[problem].getRd()\n",
    "        #userDict[user], problemDict[problem] = EloRating(userRating, problemRating, winner)\n",
    "        try:\n",
    "            userDict[user].update_player(problemRating, problemRd, bool(winner))\n",
    "            problemDict[problem].update_player(userRating, userRd, not bool(winner))\n",
    "            if user not in userGamesPlayed:\n",
    "                userGamesPlayed[user] = 1\n",
    "            else:\n",
    "                userGamesPlayed[user] = userGamesPlayed[user] + 1\n",
    "\n",
    "            newRow = [row[\"user_tactics_problem_id\"], row[\"user_hash\"], row[\"create_date\"], str(date), str(lastGame), row[\"seconds\"], row[\"is_passed\"], row[\"correct_move_count\"], row[\"rating_change\"], row[\"tactics_problem_id\"], str(int(userRating)), str(int(userRd)), str(int(problemRating)), str(int(problemRd)), str(userGamesPlayed[user]-1)]\n",
    "            rows.append(newRow)\n",
    "        except:\n",
    "            print(\"error with rating,  user:\", user)\n",
    "            print(\"problem ID:\", problem)\n",
    "            problemProblems.append(str(problem))\n",
    "\n",
    "            \n",
    "        \n",
    "    newCSV.writerows(rows)    \n",
    "    chunksUsed += 1\n",
    "    if chunksUsed % 10 == 0:\n",
    "        print(chunksUsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''newCSV = open(\"glicko_user_tactics_problem.csv_00\", \"r\").readlines()\n",
    "for line in newCSV:\n",
    "    print(line)\n",
    "'''\n",
    "#%store -r problemProblems\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(userDict)\n",
    "print(max(userGamesPlayed.values()))\n",
    "'''import operator\n",
    "sorted_x = sorted(userDict.items(), key=operator.itemgetter(1))\n",
    "sorted_x.reverse()\n",
    "sorted_x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%store -r userDict\n",
    "%store -r problemDict\n",
    "%store -r userGamesPlayed'''\n",
    "\n",
    "#now we have our variables: difficulty (problemsDict), skill (userDict), and time (time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([p.getRating() for p in userDict.values()], [500 + (i*50) for i in range(40)])\n",
    "#most people are clumped just below 1500, some are just above.\n",
    "print(\"mean rating is \", sum([p.getRating() for p in userDict.values()])/len(userDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([p.getRd() for p in userDict.values()], [0 + (i*10) for i in range(50)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(userGamesPlayed.values()), [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]) \n",
    "#probably because most people have under 5 puzzles completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([p.getRating() for p in problemDict.values()], [500 + (i*50) for i in range(40)])\n",
    "\n",
    "print(\"mean rating is \", sum([p.getRating() for p in problemDict.values()])/len(problemDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another thing to consider plotting: ELO by number of moves to solve - although not sure if that's relevant to the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename, nrows = NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df[\"is_passed\"] == 1)/len(df.index) #percentage solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userRatingData = pd.DataFrame([(p, userDict[p].getRating(), userDict[p].getRd()) for p in userDict],\n",
    "                      columns=['userHash','ratingUser', 'varianceUser'])\n",
    "problemRatingData = pd.DataFrame([(p, problemDict[p].getRating(), problemDict[p].getRd()) for p in problemDict],\n",
    "                      columns=['problemID','ratingProblem', 'varianceProblem'])\n",
    "\n",
    "df2 = pd.merge(df, userRatingData, left_on='user_hash', right_on= \"userHash\", how=\"left\")\n",
    "df2 = pd.merge(df2, problemRatingData, left_on='tactics_problem_id', right_on= \"problemID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(columns = [\"rating_change\",\"userHash\", \"problemID\" ])\n",
    "df2[1:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = df2[:int(NROWS*.8)]\n",
    "testingSet = df2[int(NROWS*.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 20, max_depth = 40)\n",
    "X = trainingSet[['seconds', 'ratingUser', 'ratingProblem', 'varianceUser', 'varianceProblem']]\n",
    "y = trainingSet['is_passed']\n",
    "rf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = testingSet[['seconds', 'ratingUser', 'ratingProblem', 'varianceUser', 'varianceProblem']]\n",
    "ytest = testingSet['is_passed']\n",
    "prediction = lr.predict(Xtest)\n",
    "print(\"all Variable RF model accuracy: \", accuracy_score(ytest, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "X = trainingSet[['seconds', 'ratingUser', 'ratingProblem', 'varianceUser', 'varianceProblem']]\n",
    "y = trainingSet['is_passed']\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = testingSet[['seconds', 'ratingUser', 'ratingProblem', 'varianceUser', 'varianceProblem']]\n",
    "ytest = testingSet['is_passed']\n",
    "prediction = lr.predict(Xtest)\n",
    "print(\"all Variable Logistic model accuracy: \", accuracy_score(ytest, prediction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "variableList = deque(['seconds', 'ratingUser', 'ratingProblem', 'varianceUser', 'varianceProblem'])\n",
    "\n",
    "\n",
    "for i in range(len(variableList)):\n",
    "    varMissing = variableList.popleft()\n",
    "    lr2 = LogisticRegression()\n",
    "    X = trainingSet[list(variableList)]\n",
    "    y = trainingSet['is_passed']\n",
    "    lr2.fit(X, y)\n",
    "    Xtest = testingSet[list(variableList)]\n",
    "    prediction = lr2.predict(Xtest)\n",
    "    print(\"No\", varMissing, \"model accuracy: \", accuracy_score(ytest, prediction))\n",
    "    variableList.append(varMissing)\n",
    "'''\n",
    "lr2 = LogisticRegression()\n",
    "X = trainingSet[['ratingUser', 'ratingProblem']]\n",
    "y = trainingSet['is_passed']\n",
    "lr2.fit(X, y)\n",
    "Xtest = testingSet[['ratingUser', 'ratingProblem']]\n",
    "prediction = lr2.predict(Xtest)\n",
    "print(\"No Time model accuracy: \", accuracy_score(ytest, prediction))\n",
    "\n",
    "lr3 = LogisticRegression()\n",
    "X = trainingSet[['seconds', 'ratingProblem']]\n",
    "y = trainingSet['is_passed']\n",
    "lr3.fit(X, y)\n",
    "Xtest = testingSet[['seconds', 'ratingProblem']]\n",
    "prediction = lr3.predict(Xtest)\n",
    "print(\"all User Rating Model accuracy: \", accuracy_score(ytest, prediction))\n",
    "\n",
    "\n",
    "lr4 = LogisticRegression()\n",
    "X = trainingSet[['seconds', 'ratingUser']]\n",
    "y = trainingSet['is_passed']\n",
    "lr4.fit(X, y)\n",
    "Xtest = testingSet[['seconds', 'ratingUser']]\n",
    "prediction = lr4.predict(Xtest)\n",
    "print(\"all Problem Rating Model accuracy: \", accuracy_score(ytest, prediction))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainingSet[['seconds', 'ratingUser', 'ratingProblem', 'varianceUser', 'varianceProblem']]\n",
    "logit = sm.Logit(y, X)\n",
    "result = logit.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(df2[\"seconds\"]), list(range(80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "ratingToAccUser = {}\n",
    "ratingToAccProblem = {}\n",
    "for index, line in df2.iterrows():\n",
    "    x += 1\n",
    "    ratingU = int(line['ratingUser'])\n",
    "    ratingP = int(line['ratingProblem'])\n",
    "    if ratingU not in ratingToAccUser:\n",
    "        ratingToAccUser[ratingU] = [0,0]\n",
    "    if ratingP not in ratingToAccProblem:\n",
    "        ratingToAccProblem[ratingP] = [0,0]\n",
    "    ratingToAccUser[ratingU][1] += 1\n",
    "    ratingToAccProblem[ratingP][1] += 1\n",
    "    ratingToAccProblem[ratingP][0] += 1\n",
    "    if line['is_passed']:\n",
    "        ratingToAccUser[ratingU][0]+= 1\n",
    "        ratingToAccProblem[ratingP][0] -= 1\n",
    "    if x %50000 == 0:\n",
    "        print(x/len(df2))\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2[\"\"]\n",
    "#test = df2.groupby(\"ratingUser\")[\"is_passed\"].mean()\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullRatingToAccUser1 = [(key, ratingToAccUser[key][0]/ratingToAccUser[key][1]) for key in ratingToAccUser]\n",
    "fullRatingToAccProblem1 = [(key, ratingToAccProblem[key][0]/ratingToAccProblem[key][1]) for key in ratingToAccProblem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frtau = pd.DataFrame(fullRatingToAccUser1, columns=['rating','accuracy'])\n",
    "frtap = pd.DataFrame(fullRatingToAccProblem1, columns=['rating','accuracy'])\n",
    "frtap = frtap.sort_values(\"rating\")\n",
    "frtau = frtau.sort_values(\"rating\")\n",
    "frtap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot('rating', 'accuracy', data = frtau)\n",
    "plt.plot('rating', 'accuracy', data = frtap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
